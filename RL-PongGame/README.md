# ğŸ“ Pong AI - Deep Q-Learning Agent

A complete implementation of Pong with a trained Deep Q-Network (DQN) agent that you can play against in real-time through a web interface. The project features WebSocket-based multiplayer, allowing humans to compete against the AI or watch AI vs AI battles.

![Python](https://img.shields.io/badge/python-3.10-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

## âœ¨ Features

- **ğŸ¤– Trained DQN Agent**: Deep Q-Learning agent trained on 84x84 grayscale frames
- **ğŸ® Real-time Multiplayer**: WebSocket-based gameplay with authoritative server
- **ğŸ‘¥ Multiple Game Modes**: Human vs AI, AI vs AI, or Human vs Human
- **ğŸ“Š Training Infrastructure**: Complete RL training pipeline with WandB integration
- **ğŸ¯ Pixel-Perfect Physics**: Deterministic game engine with consistent behavior
- **âš¡ Apple Silicon Support**: Optimized for MPS (Metal Performance Shaders)

## ğŸ¯ Demo

Play against the trained AI agent through your browser! The AI has been trained using Deep Q-Learning to master Pong gameplay.

## ğŸ“ Project Structure

```
youtube-dueling-ai-pong/
â”œâ”€â”€ frontend/                    # Web interface
â”‚   â”œâ”€â”€ index_websocket.html    # Main game UI
â”‚   â”œâ”€â”€ game_websocket.js       # Client-side game renderer
â”‚   â””â”€â”€ ws_client.js            # WebSocket communication
â”‚
â”œâ”€â”€ backend/                     # Server & game logic
â”‚   â”œâ”€â”€ server.py               # WebSocket server
â”‚   â””â”€â”€ core/
â”‚       â”œâ”€â”€ game.py             # Pong environment (Gymnasium)
â”‚       â””â”€â”€ assets.py           # Paddle & Ball physics
â”‚
â”œâ”€â”€ training/                    # RL training components
â”‚   â”œâ”€â”€ train.py               # Training script
â”‚   â”œâ”€â”€ agent.py               # DQN agent implementation
â”‚   â”œâ”€â”€ model.py               # Neural network architecture
â”‚   â”œâ”€â”€ buffer.py              # Experience replay buffer
â”‚   â””â”€â”€ checkpoint.py          # Model checkpointing
â”‚
â”œâ”€â”€ models/                      # Trained models
â”‚   â””â”€â”€ latest.pt              # Latest DQN checkpoint
â”‚
â”œâ”€â”€ runs/                        # TensorBoard logs
â”œâ”€â”€ wandb/                       # Weights & Biases logs
â””â”€â”€ requirements.txt            # Python dependencies
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- conda (recommended) or virtualenv
- Modern web browser

### 1. Environment Setup

```bash
# Clone the repository
cd youtube-dueling-ai-pong

# Create conda environment
conda create -n game_rl python=3.10
conda activate game_rl

# Install dependencies
pip install -r requirements.txt
```

### 2. Start the Game Server

```bash
# Activate environment
conda activate game_rl

# Start WebSocket server
python backend/server.py
```

The server will:
- Load the trained AI model from `models/latest.pt`
- Start WebSocket server on `ws://localhost:8765`
- Wait for browser connections

### 3. Launch the Frontend

In a separate terminal:

```bash
# Start HTTP server
python -m http.server 8000
```

Then open in your browser:
```
http://localhost:8000/frontend/index_websocket.html
```

## ğŸ® How to Play

### Controls

**Player 1 (Green - Right Paddle)**
- **â†‘** Arrow Up: Move up
- **â†“** Arrow Down: Move down

**Player 2 (Purple - Left Paddle)**
- **W**: Move up
- **S**: Move down

**General**
- **R**: Reset game

### Game Modes

Click the buttons in the UI to switch player modes:

- **ğŸ‘¤ Human**: Manual control with keyboard
- **ğŸ§  AI (DQN)**: Trained Deep Q-Network agent

**Try These Combinations:**
- **Human vs AI**: Challenge the trained agent
- **AI vs AI**: Watch two agents compete
- **Human vs Human**: Local multiplayer

### Rules

- First player to score **20 points** wins
- Ball speeds up slightly with each paddle hit
- Game physics run at 30 FPS for balanced gameplay

## ğŸ§  Training Your Own Agent

### Train from Scratch

```bash
conda activate game_rl
python training/train.py
```

### Training Configuration

Key hyperparameters (in `train.py`):

```python
episodes = 10000              # Total training episodes
learning_rate = 0.0001        # Adam optimizer LR
gamma = 0.99                  # Discount factor
epsilon = 1.0                 # Initial exploration
epsilon_decay = 0.995         # Exploration decay
batch_size = 32               # Replay buffer batch
frame_stack = 3               # Stacked frames
hidden_dim = 756              # Network hidden size
```

### Model Architecture

```
Input: 3 Ã— 84 Ã— 84 (3 stacked grayscale frames)
  â†“
Conv2D(32, kernel=8, stride=4) + ReLU
  â†“
Conv2D(64, kernel=4, stride=2) + ReLU
  â†“
Conv2D(64, kernel=3, stride=1) + ReLU
  â†“
Flatten â†’ FC(756) + ReLU
  â†“
Output: FC(3) â†’ [Up, Down, Stay]
```

### Monitoring Training

Training metrics are logged to:
- **WandB**: Real-time training dashboard
- **TensorBoard**: Local metric visualization

```bash
# View TensorBoard logs
tensorboard --logdir runs/
```

## ğŸ”§ Server Configuration

### Command Line Options

```bash
python backend/server.py --help

Options:
  --host HOST          Host to bind to (default: 0.0.0.0)
  --port PORT          Port to bind to (default: 8765)
  --model PATH         Path to trained model (default: models/latest.pt)
```

### Custom Model

```bash
python backend/server.py --model path/to/your/model.pt
```

## ğŸ“Š Game Environment

### Observation Space
- **Type**: Image (84Ã—84 grayscale)
- **Channels**: 3 (frame stacking)
- **Preprocessing**: Downscale â†’ Grayscale â†’ Binary (0 or 255)

### Action Space
- **Type**: Discrete(3)
- **Actions**:
  - `0`: Stay
  - `1`: Move up
  - `2`: Move down

### Reward Structure
- **+1**: Score a point
- **-1**: Opponent scores
- **0**: Otherwise

### Episode Termination
- Game ends when either player reaches 20 points
- Maximum episode steps: 1000

## ğŸŒ WebSocket Protocol

### Client â†’ Server

```json
// Reset game
{ "type": "reset" }

// Send action
{ "type": "action", "player": 1, "action": 0 }

// Change mode
{ "type": "mode", "player": 1, "mode": "ai" }

// Request state
{ "type": "get_state" }
```

### Server â†’ Client

```json
// Game state update
{
  "type": "state",
  "ball": { "x": 640, "y": 480, "vx": 5, "vy": 3 },
  "paddle1": { "y": 420 },
  "paddle2": { "y": 380 },
  "score1": 5,
  "score2": 3,
  "done": false
}
```

## ğŸ› ï¸ Technical Details

### Technologies Used

**Backend:**
- PyTorch (Deep Learning)
- Gymnasium (RL Environment)
- Pygame (Game Engine)
- WebSockets (Real-time Communication)
- WandB (Experiment Tracking)

**Frontend:**
- HTML5 Canvas (Rendering)
- Vanilla JavaScript (Game Logic)
- WebSocket API (Server Communication)

### Key Design Decisions

1. **Authoritative Server**: Game logic runs on Python backend to ensure AI sees the same environment as training
2. **Frame Stacking**: Uses 3 consecutive frames to capture motion/velocity information
3. **Coordinate Preservation**: Game runs at 1280Ã—960 internally to match training environment
4. **30 FPS Gameplay**: Balanced speed for both human and AI players

## ğŸ› Troubleshooting

### AI Not Loading

```bash
# Check if model file exists
ls -lh models/latest.pt

# Verify model architecture matches
# Default hidden_dim: 756
```

### Connection Issues

```bash
# Check if port 8765 is available
lsof -i :8765

# Check if HTTP server is running
lsof -i :8000
```

### Performance Issues

- Ensure MPS is available (Apple Silicon): `torch.backends.mps.is_available()`
- Reduce FPS in `server.py` if needed
- Close other resource-intensive applications

## ğŸ“ˆ Training Results

The included model (`models/latest.pt`) was trained with:
- **Architecture**: 3-layer CNN + 2 FC layers
- **Hidden Dimension**: 756
- **Training Time**: ~10,000 episodes
- **Framework**: DQN with experience replay
- **Performance**: Competitive against hard bot

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:

- [ ] Implement PPO/A3C algorithms
- [ ] Add self-play training
- [ ] Multi-agent tournaments
- [ ] Spectator mode
- [ ] Save/replay matches
- [ ] Leaderboard system

## ğŸ“ License

MIT License - feel free to use for learning and projects!

## ğŸ™ Acknowledgments

- Built using OpenAI Gymnasium framework
- Inspired by classic Pong and modern RL techniques
- Training infrastructure based on DQN research

## ğŸ“§ Contact

For questions or suggestions, please open an issue on GitHub.

---

**Enjoy playing against your AI! ğŸ®ğŸ¤–**
